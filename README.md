# ğŸ“¦ Box-Embedding-Unboxed
> **Visualizing Geometric Reasoning & Topological Constraints in 2D Space**


*(ì—¬ê¸°ì— ê°€ì¥ ì˜ ë‚˜ì˜¨ ìµœì¢… ì„±ê³µ GIF ê²½ë¡œë¥¼ ë„£ìœ¼ì„¸ìš”)*

## 1. Introduction
**"Why Box embedding?"**

In KG(Knowledge Graph), representing a knowledge(entity) as a point-vector in the embedding space makes hard to answer queries over the KG. It lacks the ability to represent Hierarchy and Uncertainty of knowledge. 

For example, think of answering complex queries involving sets of entities(e.g., *â€œwhere did Canadian citizens with
Turing Award graduate?â€*) where each entities are represented as point-vector in the embedding space, It would be hard to imagine how answer point-vector of the query should be like. [(Ren et al., 2020)](https://arxiv.org/pdf/2002.05969)

This project visualizes how model train the Hierarchy and Uncentainty of knowledge using geometric properties of Box embedding[(Vilnis et al., 208)](https://arxiv.org/pdf/2109.04997)


## 2. Key Features
* **Geometric Reasoning:** Implement intersection and containment of knowledge in 2D space using box
* **Custom Synthetic Dataset:** Tree-structure dataset consist of Mutual-Exclusive 3 domain(Science, Art, Business)
* **Optimization:**
    * **Volume Regularization:** : Prevent boxes being bigger without constraint
    * **Aspect Ratio Regularization:** Prvent boxes making Orthogonal Overlap

## 3. Experiment & Analysis (Trouble Shooting)
The main point of this project is solving Topological trap with "Data-centric" method and dealing with several optimization problem

### 3.1. The Limitation of 2D Space (Blocking)
* **Problem:** 2D plane(which I choose willfully to visualize box embedding) has unsufficient bypass compare to high-dimension. Thus, If there is an obstacle between the parent and the child
 2ì°¨ì› í‰ë©´ì€ ê³ ì°¨ì›ì— ë¹„í•´ ìš°íšŒë¡œê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ì´ë¡œ ì¸í•´ ì§ê³„ ë¶€ëª¨(`Jazz` â†’ `Music`) ê´€ê³„ëŠ” í•™ìŠµë˜ì§€ë§Œ, ì¡°ìƒ(`Jazz` â†’ `Art`) ê´€ê³„ëŠ” ì¤‘ê°„ì˜ ì¥ì• ë¬¼(Negative Samples)ì— ê°€ë¡œë§‰í˜€ ìˆ˜ë ´í•˜ì§€ ëª»í•˜ëŠ” **Blocking** í˜„ìƒì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤.
* **Solution (Data-Centric Approach):**
    * ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ íŠœë‹í•˜ëŠ” ëŒ€ì‹ , **Transitive Closure (ì´í–‰ì  íí¬)** ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í–ˆìŠµë‹ˆë‹¤.
    * ë°ì´í„°ì…‹ì— `(Grandchild, IsA, Grandparent)` ê´€ê³„ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì£¼ì…í•˜ì—¬, ëª¨ë¸ì´ ì¤‘ê°„ ì¥ì• ë¬¼ì„ ë›°ì–´ë„˜ì–´ ìˆ˜ë ´í•˜ë„ë¡ ìœ ë„í–ˆìŠµë‹ˆë‹¤.

### 3.2 Greedy box problem : Add loss to big box
### 3.3.Boxes play tricks :  Anisotropy (ë¹„ë“±ë°©ì„±)
* **Observation:** íŠ¹ì • ë°•ìŠ¤ë“¤ì´ ì„¸ë¡œ í˜¹ì€ ê°€ë¡œë¡œ ê¸¸ê²Œ ëŠ˜ì–´ì§€ëŠ” í˜„ìƒ ê´€ì¸¡.
* **Analysis:** ì´ëŠ” ëª¨ë¸ì´ Negative Constraintê°€ ì—†ëŠ” ë°©í–¥(Null Space)ìœ¼ë¡œ ë°•ìŠ¤ë¥¼ í™•ì¥í•˜ì—¬ Lossë¥¼ ì¤„ì´ë ¤ëŠ” ê¸°í•˜í•™ì  ìµœì í™” ê³¼ì •ì„ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.
### 3.4 Sometimes, Hate is useful : The necessity of negative sampling

## 4. Conclusion
**"Better Data > Better Model"**
ì´ˆê¸°ì—ëŠ” Learning Rateë‚˜ Margin íŠœë‹ì— ì§‘ì¤‘í–ˆìœ¼ë‚˜, ê·¼ë³¸ì ì¸ í•´ê²°ì±…ì€ **ë°ì´í„°ì˜ êµ¬ì¡°ì  ê²°í•¨(Transitivity ë¶€ì¡±)ì„ ë³´ì™„**í•˜ëŠ” ê²ƒì´ì—ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ AI ëª¨ë¸ë§ì—ì„œ ì•„í‚¤í…ì²˜ë§Œí¼ì´ë‚˜ **ë°ì´í„°ì˜ í’ˆì§ˆê³¼ êµ¬ì¡°(Data Quality)**ê°€ ì„±ëŠ¥ì— ê²°ì •ì ì„ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.

"Optimization Strategy: To handle the sparsity of hierarchical data (frequent root nodes vs. rare leaf nodes), I utilized the Adam optimizer, which adapts the learning rate for each embedding parameter individually, preventing frequent nodes from oscillating while ensuring rare nodes converge effectively."

## 5. Tech Stack
* **Language:** Python 3.10
* **Framework:** PyTorch
* **Visualization:** Matplotlib, ImageIO



## 6. How to Run
```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Train & Visualize
python main.py --epochs 3000 --lr 0.005